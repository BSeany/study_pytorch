# 什么是梯度？
PyTorch 中的梯度 = 「损失函数对每个参数的偏导数」按参数的形状 “一一对应” 存储在张量中，单参数是标量张量、多参数是向量张量、矩阵参数是矩阵张量，本质就是偏导数的 “结构化集合”。

正因为梯度是 “按参数形状存储偏导数”，所以：
训练时必须用 optimizer.zero_grad() 清零梯度 —— 因为 PyTorch 会累加梯度（多次反向传播不清零，偏导数会叠加），这是工程实现的特性，和纯数学的 “梯度是某一点的静态值” 不同；
梯度的形状永远和参数一致 —— 比如参数 W 是 [5, 10] 的矩阵，W.grad 也一定是 [5, 10]，每个位置 W.grad[i][j] 就是 ∂Loss/∂W[i][j]。
最终总结

核心对应：PyTorch 梯度张量的每个元素 = 数学上损失函数对对应位置参数的偏导数；
存储形式：梯度的形状和参数完全一致（标量 / 向量 / 矩阵 / 高维张量），本质是偏导数的结构化存储；
工程特性：梯度会累加，训练时需手动清零，这是框架实现的细节，而非数学定义。

## 怎样计算梯度？
grad 是 gradient 的缩写，也就是梯度的意思
```
# 设置自动求导
x = torch.randn(2, 2, requires_grad=True)
result = x * 2

# 触发「反向传播」，自动计算当前张量对所有依赖的、
# 开启梯度追踪（requires_grad=True）的参数的梯度，并把梯度存储到这些参数的 .grad 属性中。
result.backward()
print(result.grad)  # 打印梯度

```